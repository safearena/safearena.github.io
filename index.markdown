---
layout: default
---

<div align="center">

<h1>SafeArena</h1>
<h2>Evaluating Safety in Autonomous Web Agents</h2>

Ada Tur<sup>*</sup> &nbsp; Nicholas Meade<sup>*</sup> &nbsp; Xing Han LÃ¹<sup>*</sup> &nbsp; Alejandra Zambrano<sup>â€ </sup> &nbsp; Arkil Patel<sup>â€ </sup>
<br>
&nbsp; Esin Durmus &nbsp; Spandana Gella &nbsp; Karolina StaÅ„czak &nbsp; Siva Reddy
<br>
<br>
<sup>*</sup>Equal Contribution

<table>
      <tr>
            <td>
                  <a href="https://github.com/McGill-NLP/safearena">ðŸ’» GitHub</a>
            </td>
            <td>
                  <a href="404.html">ðŸ“„ Paper</a>
            </td>
            <td>
                  <a href="404.html">ðŸ“Š Leaderboard</a>
            </td>
      </tr>
</table>

</div>

## Are Web Agents Safe?

LLM-based agents are becoming increasingly proficient at solving web-based tasks. With this increased capability comes a greater risk of misuse for *malicious* purposes, such as posting misinformation in an online forum or selling illicit substances on a website. To evaluate these risks, we propose **SafeArena**, the first benchmark to focus on the deliberate misuse of web agents.

<img src="/assets/safearena_fig1_v2.jpg" alt="SafeArena Fig1">

## SafeArena

**SafeArena** comprises 250 safe and 250 harmful tasks across four websites. We classify the harmful tasks into five harm categories---*misinformation*, *illegal activity*, *harassment*, *cybercrime*, and *social bias*---designed to assess realistic misuses of web agents.

# Evaluating Agent Safety

We evaluate agents under three metrics: Task Completion Rate, Refusal Rate, and Normalized Safety Score. Task Completion Rate is a binary reward assigned on task completion; Refusal Rate is computed with a string-based refusal detector; Normalized Safety Score is computed using the following formula:
<br>
$$
\text{NSS} = 1 - \frac{\sum_{i=1}^N \mathcal{R}(S_i) \times \mathcal{R}(H_i)}{\sum_{i=1}^N \mathcal{R}(S_i)}
$$

## How Do Current Agents Perform?

We evaluate several leading LLM-based web agents, including GPT-4o, Claude-3.5 Sonnet, Qwen-2 72B, and Llama-3.2 90B, on our benchmark. 

Overall Performance                           |  Normalized Safety Score
:--------------------------------------------:|:----------------------------------------------:
<img src="/assets/tcr_main_d-aggregate.jpg" width=500px>  |  <img src="/assets/tcr_main_d-normalized_safety.jpg" width=500px>

We find that agents are surprisingly compliant with malicious requests, with GPT-4o and Qwen-2 completing 22.8% and 26.0% of the harmful intents, respectively. Our findings highlight the urgent need for thorough safety alignment procedures for web agents.

## Which Harm Categories Are Agents Most Vulnerable Against?

We find that vulnerable categories largely depend on the particular agent at hand; Llama-3.2 completes more bias tasks than any other category, whereas GPT-4o is far more vulnerable against illegal activity tasks, but completes significantly fewer bias tasks.

<img src="/assets/tcr_radar_d-category_v2.jpg" width=500px>

## How Can I Use The Benchmark?

We make the benchmark, as well as all relevant code publicly available at [github.com/McGill-NLP/safearena](https://github.com/McGill-NLP/safearena). We also host the SafeArena [leaderboard](https://huggingface.co/spaces/McGill-NLP/safearena-leaderboard); to submit to the leaderboard, create an *issue* in the GitHub repo above with the tag **[Leaderboard]**.

## Citing SafeArena

Please use the following BibTex citation when using any material from SafeArena:
<br>
```
@misc{safearena2025,
      title={SafeArena: Evaluating Safety in Autonomous Web Agents}, 
      author={Ada Tur and Nicholas Meade and Xing Han LÃ¹ and Alejandra Zambrano and
              Arkil Patel and Esin Durmus and Spandana Gella and Karolina StaÅ„czak and Siva Reddy},
      year={2025},
      eprint={TBD},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

